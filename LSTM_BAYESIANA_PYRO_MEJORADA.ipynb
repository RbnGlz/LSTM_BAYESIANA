{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f03801-10dc-476e-b473-f0c2b763f3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar bibliotecas necesarias\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import multiprocessing\n",
    "from functools import partial\n",
    "import os\n",
    "import time\n",
    "import concurrent.futures\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Configurar estilo de visualización\n",
    "plt.style.use('seaborn-v0_8-darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d8f1ed-0fd3-4c63-bcea-e0cf8ca11c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1️⃣ Descargar datos de precios de acciones - OPTIMIZADO\n",
    "def get_stock_data(ticker=\"AAPL\", start=\"2020-01-01\", end=\"2024-01-01\", cache_dir=\"data_cache\"):\n",
    "    \"\"\"\n",
    "    Descarga datos históricos de precios de acciones con caché y manejo mejorado de errores.\n",
    "    \n",
    "    Args:\n",
    "        ticker: Símbolo de la acción o lista de símbolos\n",
    "        start: Fecha de inicio (formato YYYY-MM-DD)\n",
    "        end: Fecha de fin (formato YYYY-MM-DD)\n",
    "        cache_dir: Directorio para almacenar datos en caché\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame con datos de precios o diccionario de DataFrames si se pasan múltiples tickers\n",
    "    \"\"\"\n",
    "    # Crear directorio de caché si no existe\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    \n",
    "    # Función para descargar un solo ticker con reintentos\n",
    "    def download_single_ticker(tick):\n",
    "        cache_file = os.path.join(cache_dir, f\"{tick}_{start}_{end}.parquet\")\n",
    "        \n",
    "        # Verificar si existe en caché\n",
    "        if os.path.exists(cache_file):\n",
    "            try:\n",
    "                return pd.read_parquet(cache_file)\n",
    "            except Exception:\n",
    "                # Si hay error al leer caché, descargar de nuevo\n",
    "                pass\n",
    "        \n",
    "        # Implementar reintentos con backoff exponencial\n",
    "        max_retries = 5\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                df = yf.download(tick, start=start, end=end, progress=False)\n",
    "                \n",
    "                if df.empty:\n",
    "                    raise ValueError(f\"No se encontraron datos para {tick}\")\n",
    "                \n",
    "                # Guardar en caché\n",
    "                df.to_parquet(cache_file)\n",
    "                return df\n",
    "            except Exception as e:\n",
    "                if attempt == max_retries - 1:\n",
    "                    print(f\"Error al descargar {tick}: {e}\")\n",
    "                    raise\n",
    "                # Esperar con backoff exponencial\n",
    "                time.sleep(2 ** attempt)\n",
    "    \n",
    "    # Manejar múltiples tickers o un solo ticker\n",
    "    if isinstance(ticker, list):\n",
    "        # Descargar en paralelo\n",
    "        results = {}\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=min(len(ticker), 10)) as executor:\n",
    "            future_to_ticker = {executor.submit(download_single_ticker, tick): tick for tick in ticker}\n",
    "            for future in concurrent.futures.as_completed(future_to_ticker):\n",
    "                tick = future_to_ticker[future]\n",
    "                try:\n",
    "                    results[tick] = future.result()\n",
    "                except Exception as e:\n",
    "                    print(f\"Error en ticker {tick}: {e}\")\n",
    "        return results\n",
    "    else:\n",
    "        # Descargar un solo ticker\n",
    "        df = download_single_ticker(ticker)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c7d1e0-8948-496f-b570-929a12199e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2️⃣ Preprocesar datos - OPTIMIZADO\n",
    "def prepare_data(data, seq_length=30, test_size=0.2, val_size=0.1, stride=1, feature_engineering=True):\n",
    "    \"\"\"\n",
    "    Preprocesa los datos para el entrenamiento del modelo LSTM con técnicas avanzadas.\n",
    "    \n",
    "    Args:\n",
    "        data: DataFrame con datos de precios\n",
    "        seq_length: Longitud de la secuencia para predecir\n",
    "        test_size: Proporción de datos para prueba\n",
    "        val_size: Proporción de datos para validación\n",
    "        stride: Paso entre secuencias consecutivas (1 = todas las secuencias)\n",
    "        feature_engineering: Si se deben añadir características técnicas\n",
    "        \n",
    "    Returns:\n",
    "        Tensores de entrenamiento/validación/prueba y scaler para desnormalizar\n",
    "    \"\"\"\n",
    "    # Trabajar con un DataFrame\n",
    "    if isinstance(data, np.ndarray):\n",
    "        df = pd.DataFrame(data, columns=['Close'])\n",
    "    else:\n",
    "        df = data.copy()\n",
    "        if 'Close' not in df.columns:\n",
    "            raise ValueError(\"El DataFrame debe contener una columna 'Close'\")\n",
    "    \n",
    "    # Utilizar solo datos de cierre para mantener compatibilidad con código original\n",
    "    if feature_engineering:\n",
    "        # Añadir características técnicas (retornos, medias móviles, etc.)\n",
    "        # Retornos logarítmicos (más estables que los porcentuales)\n",
    "        df['log_return'] = np.log(df['Close'] / df['Close'].shift(1))\n",
    "        \n",
    "        # Medias móviles\n",
    "        df['ma7'] = df['Close'].rolling(window=7).mean() / df['Close'] - 1\n",
    "        df['ma21'] = df['Close'].rolling(window=21).mean() / df['Close'] - 1\n",
    "        \n",
    "        # Volatilidad\n",
    "        df['volatility'] = df['log_return'].rolling(window=21).std()\n",
    "        \n",
    "        # Momentum\n",
    "        df['momentum'] = df['Close'].pct_change(periods=5)\n",
    "        \n",
    "        # Eliminar NaN\n",
    "        df = df.dropna()\n",
    "    \n",
    "    # Seleccionar columnas a usar\n",
    "    if feature_engineering:\n",
    "        feature_columns = ['Close', 'log_return', 'ma7', 'ma21', 'volatility', 'momentum']\n",
    "        features = df[feature_columns].values\n",
    "    else:\n",
    "        feature_columns = ['Close']\n",
    "        features = df[['Close']].values\n",
    "    \n",
    "    # Normalizar datos - usando RobustScaler para mayor robustez ante outliers\n",
    "    scaler = RobustScaler()\n",
    "    scaled_features = scaler.fit_transform(features)\n",
    "    \n",
    "    # Crear secuencias X e Y de manera eficiente\n",
    "    X, y = [], []\n",
    "    \n",
    "    # Método vectorizado para crear secuencias (mucho más rápido que los bucles)\n",
    "    for i in range(0, len(scaled_features) - seq_length, stride):\n",
    "        X.append(scaled_features[i:i + seq_length])\n",
    "        y.append(scaled_features[i + seq_length, 0])  # Predecir solo el precio de cierre\n",
    "    \n",
    "    # Convertir a arrays NumPy eficientemente\n",
    "    X = np.array(X)\n",
    "    y = np.array(y).reshape(-1, 1)\n",
    "    \n",
    "    # Determinar tamaños de conjuntos\n",
    "    n = len(X)\n",
    "    test_idx = int(n * (1 - test_size))\n",
    "    val_idx = int(n * (1 - test_size - val_size))\n",
    "    \n",
    "    # Dividir en conjuntos de entrenamiento, validación y prueba\n",
    "    # Mantener tiempo cronológico (sin aleatorizar)\n",
    "    X_train, y_train = X[:val_idx], y[:val_idx]\n",
    "    X_val, y_val = X[val_idx:test_idx], y[val_idx:test_idx]\n",
    "    X_test, y_test = X[test_idx:], y[test_idx:]\n",
    "    \n",
    "    # Convertir a tensores PyTorch\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "    X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "    y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "    \n",
    "    return (X_train, y_train), (X_val, y_val), (X_test, y_test), scaler, feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ad8200-5c30-4aab-9a9d-33da3a4eb299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3️⃣ Definir el modelo LSTM mejorado\n",
    "class BayesianLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Utilizar bidireccional para capturar mejor las tendencias\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size, \n",
    "            hidden_size, \n",
    "            num_layers, \n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=False  # Cambiar a True si se necesita bidireccionalidad\n",
    "        )\n",
    "        \n",
    "        # Añadir capa de normalización para estabilizar entrenamiento\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Inicializar estados ocultos con una mejor inicialización\n",
    "        batch_size = x.size(0)\n",
    "        device = x.device\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=device)\n",
    "        \n",
    "        # Optimización de memoria\n",
    "        with torch.set_grad_enabled(self.training):\n",
    "            out, _ = self.lstm(x, (h0, c0))\n",
    "            out = self.norm(out[:, -1, :])  # Normalizar antes de la capa lineal\n",
    "            out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff91bf34-69e8-424a-8f01-5434495c1b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4️⃣ Modelo probabilístico mejorado en Pyro\n",
    "def pyro_model(x, y=None, model=None):\n",
    "    \"\"\"\n",
    "    Define el modelo probabilístico para inferencia bayesiana con priors más informativos.\n",
    "    \"\"\"\n",
    "    if model is None:\n",
    "        raise ValueError(\"Se debe proporcionar un modelo\")\n",
    "    \n",
    "    # Priors para pesos y sesgos - más informativo para convergencia más rápida\n",
    "    scale_prior = 0.1\n",
    "    \n",
    "    # Priors para la capa final\n",
    "    w_prior = dist.Normal(0.0, scale_prior).expand(model.fc.weight.shape).to_event(2)\n",
    "    b_prior = dist.Normal(0.0, scale_prior).expand(model.fc.bias.shape).to_event(1)\n",
    "    \n",
    "    # También podemos añadir priors para los parámetros LSTM\n",
    "    priors = {\n",
    "        \"fc.weight\": w_prior, \n",
    "        \"fc.bias\": b_prior,\n",
    "    }\n",
    "    \n",
    "    # Módulo aleatorio con los priors\n",
    "    lifted_module = pyro.random_module(\"module\", model, priors)()\n",
    "    \n",
    "    # Predicción del modelo\n",
    "    y_hat = lifted_module(x)\n",
    "\n",
    "    # Prior para la varianza del ruido - más informativo\n",
    "    sigma = pyro.sample(\"sigma\", dist.Gamma(2.0, 3.0))\n",
    "    \n",
    "    # Likelihood con mejor manejo de forma\n",
    "    with pyro.plate(\"data\", x.size(0)):\n",
    "        pyro.sample(\"obs\", dist.Normal(y_hat, sigma), obs=y)\n",
    "\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70025047-58d7-4ef2-a0b0-4fb735e567ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5️⃣ Entrenar con inferencia bayesiana y early stopping\n",
    "def train(model, x_train, y_train, x_val=None, y_val=None, num_epochs=1000, batch_size=64, lr=0.01, \n",
    "          patience=20, early_stopping=True):\n",
    "    \"\"\"\n",
    "    Entrena el modelo usando SVI (Stochastic Variational Inference) con early stopping.\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo LSTM\n",
    "        x_train, y_train: Datos de entrenamiento\n",
    "        x_val, y_val: Datos de validación (opcional)\n",
    "        num_epochs: Número de épocas\n",
    "        batch_size: Tamaño del lote\n",
    "        lr: Tasa de aprendizaje\n",
    "        patience: Número de épocas para early stopping\n",
    "        early_stopping: Si se debe utilizar early stopping\n",
    "    \n",
    "    Returns:\n",
    "        Lista de pérdidas durante el entrenamiento y la guía entrenada\n",
    "    \"\"\"\n",
    "    # Limpiar parámetros anteriores\n",
    "    pyro.clear_param_store()\n",
    "    \n",
    "    # Crear dataset y dataloader para procesamiento por lotes\n",
    "    dataset = TensorDataset(x_train, y_train)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Si hay datos de validación, crear dataloader\n",
    "    val_dataloader = None\n",
    "    if x_val is not None and y_val is not None:\n",
    "        val_dataset = TensorDataset(x_val, y_val)\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    \n",
    "    # Configurar guía y optimizador\n",
    "    guide = pyro.infer.autoguide.AutoNormal(lambda x, y: pyro_model(x, y, model))\n",
    "    optimizer = Adam({\"lr\": lr})\n",
    "    svi = SVI(\n",
    "        model=lambda x, y: pyro_model(x, y, model),\n",
    "        guide=guide,\n",
    "        optim=optimizer,\n",
    "        loss=Trace_ELBO()\n",
    "    )\n",
    "\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    no_improvement = 0\n",
    "    best_state = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Entrenamiento\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for batch_x, batch_y in dataloader:\n",
    "            loss = svi.step(batch_x, batch_y)\n",
    "            epoch_loss += loss\n",
    "        \n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        losses.append(avg_loss)\n",
    "        \n",
    "        # Validación (si hay datos)\n",
    "        if val_dataloader is not None:\n",
    "            model.eval()\n",
    "            val_epoch_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for batch_x, batch_y in val_dataloader:\n",
    "                    # Calcular loss en validación\n",
    "                    val_loss = svi.evaluate_loss(batch_x, batch_y)\n",
    "                    val_epoch_loss += val_loss\n",
    "            \n",
    "            avg_val_loss = val_epoch_loss / len(val_dataloader)\n",
    "            val_losses.append(avg_val_loss)\n",
    "            \n",
    "            # Imprimir progreso\n",
    "            if epoch % 50 == 0:\n",
    "                print(f\"Época {epoch}/{num_epochs}, Pérdida: {avg_loss:.4f}, Val: {avg_val_loss:.4f}\")\n",
    "            \n",
    "            # Early stopping\n",
    "            if early_stopping:\n",
    "                if avg_val_loss < best_val_loss:\n",
    "                    best_val_loss = avg_val_loss\n",
    "                    no_improvement = 0\n",
    "                    # Guardar el mejor estado\n",
    "                    best_state = guide\n",
    "                else:\n",
    "                    no_improvement += 1\n",
    "                \n",
    "                if no_improvement >= patience:\n",
    "                    print(f\"Early stopping en época {epoch}\")\n",
    "                    return losses, best_state if best_state is not None else guide\n",
    "        else:\n",
    "            if epoch % 50 == 0:\n",
    "                print(f\"Época {epoch}/{num_epochs}, Pérdida: {avg_loss:.4f}\")\n",
    "\n",
    "    return losses, guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedb5f3a-7dc5-497e-a308-78a02ec222c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6️⃣ Predicción optimizada con el modelo entrenado\n",
    "def predict(model, x_input, guide, num_samples=100, use_parallel=True):\n",
    "    \"\"\"\n",
    "    Realiza predicciones con múltiples muestras del modelo posterior.\n",
    "    Implementación optimizada para CPU y GPU.\n",
    "    \"\"\"\n",
    "    device = x_input.device\n",
    "    predictions = []\n",
    "    \n",
    "    if use_parallel and device.type == 'cpu':\n",
    "        # Solo usar paralelización si estamos en CPU\n",
    "        num_cores = min(multiprocessing.cpu_count(), num_samples)\n",
    "        \n",
    "        # Mover datos a CPU para multiprocessing\n",
    "        x_cpu = x_input.cpu()\n",
    "        \n",
    "        # Función local para predicción que maneje bien el contexto de device\n",
    "        def _predict_sample_wrapper(i):\n",
    "            torch.manual_seed(i)  # Garantizar diferentes muestras\n",
    "            sampled_model = guide()\n",
    "            sampled_model.to('cpu')\n",
    "            with torch.no_grad():\n",
    "                return sampled_model(x_cpu).numpy()\n",
    "        \n",
    "        with multiprocessing.Pool(num_cores) as pool:\n",
    "            predictions = pool.map(_predict_sample_wrapper, range(num_samples))\n",
    "    else:\n",
    "        # Ejecución secuencial (mejor para GPU)\n",
    "        for i in range(num_samples):\n",
    "            torch.manual_seed(i)  # Garantizar diferentes muestras\n",
    "            sampled_model = guide()\n",
    "            sampled_model.to(device)\n",
    "            with torch.no_grad():\n",
    "                pred = sampled_model(x_input).cpu().numpy()\n",
    "                predictions.append(pred)\n",
    "    \n",
    "    predictions = np.array(predictions)\n",
    "    mean_pred = np.mean(predictions, axis=0)\n",
    "    std_pred = np.std(predictions, axis=0)\n",
    "    \n",
    "    return mean_pred, std_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a353f995-89f8-40a6-a6ae-fd0aeeadd824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7️⃣ Evaluar el modelo con corrección matemática\n",
    "def evaluate_model(model, guide, x_test, y_test, scaler, feature_columns):\n",
    "    \"\"\"\n",
    "    Evalúa el modelo en el conjunto de prueba con cálculos de error corregidos.\n",
    "    \"\"\"\n",
    "    # Determinar si usar paralelización\n",
    "    device = x_test.device\n",
    "    use_parallel = device.type == 'cpu'\n",
    "    \n",
    "    mean_preds, std_preds = predict(model, x_test, guide, use_parallel=use_parallel)\n",
    "    y_test_np = y_test.cpu().numpy()\n",
    "\n",
    "    # Ajustar forma para inverse_transform\n",
    "    mean_preds_reshaped = mean_preds.reshape(-1, 1)\n",
    "    y_test_reshaped = y_test_np.reshape(-1, 1)\n",
    "\n",
    "    # Crear arrays para transformación inversa\n",
    "    # Si tenemos múltiples características, necesitamos crear arrays completos\n",
    "    if len(feature_columns) > 1:\n",
    "        # Crear array dummy con ceros para las otras características\n",
    "        dummy_array = np.zeros((mean_preds_reshaped.shape[0], len(feature_columns)))\n",
    "        # Colocar valores reales solo en la columna 'Close'\n",
    "        close_idx = feature_columns.index('Close')\n",
    "        dummy_array[:, close_idx] = mean_preds_reshaped.flatten()\n",
    "        mean_preds_for_inverse = dummy_array\n",
    "        \n",
    "        # Lo mismo para los valores reales y std\n",
    "        dummy_y = np.zeros((y_test_reshaped.shape[0], len(feature_columns)))\n",
    "        dummy_y[:, close_idx] = y_test_reshaped.flatten()\n",
    "        y_test_for_inverse = dummy_y\n",
    "        \n",
    "        # Para std_preds\n",
    "        dummy_std = np.zeros((mean_preds_reshaped.shape[0], len(feature_columns)))\n",
    "        dummy_std[:, close_idx] = std_preds.flatten()\n",
    "        std_preds_for_inverse = dummy_std\n",
    "    else:\n",
    "        mean_preds_for_inverse = mean_preds_reshaped\n",
    "        y_test_for_inverse = y_test_reshaped\n",
    "        std_preds_for_inverse = std_preds.reshape(-1, 1)\n",
    "    \n",
    "    # Corrección matemática para desnormalizar desviación estándar\n",
    "    if isinstance(scaler, RobustScaler):\n",
    "        # Para RobustScaler, necesitamos saber el factor de escala (IQR)\n",
    "        # Aproximar usando la diferencia entre un valor con y sin std\n",
    "        dummy_zeros = np.zeros_like(std_preds_for_inverse)\n",
    "        base = scaler.inverse_transform(dummy_zeros)\n",
    "        with_std = scaler.inverse_transform(std_preds_for_inverse)\n",
    "        std_preds_real = with_std - base\n",
    "        \n",
    "        # Desnormalizar predicciones y valores reales\n",
    "        mean_preds_real = scaler.inverse_transform(mean_preds_for_inverse)\n",
    "        y_test_real = scaler.inverse_transform(y_test_for_inverse)\n",
    "        \n",
    "        # Extraer solo la columna de Close\n",
    "        if len(feature_columns) > 1:\n",
    "            mean_preds_real = mean_preds_real[:, close_idx].reshape(-1, 1)\n",
    "            y_test_real = y_test_real[:, close_idx].reshape(-1, 1)\n",
    "            std_preds_real = std_preds_real[:, close_idx].reshape(-1, 1)\n",
    "    else:\n",
    "        # Si no es RobustScaler, asumir MinMaxScaler u otro\n",
    "        scale_factor = 1.0  # Ajustar según el scaler\n",
    "        std_preds_real = std_preds.reshape(-1, 1) * scale_factor\n",
    "        mean_preds_real = scaler.inverse_transform(mean_preds_for_inverse)\n",
    "        y_test_real = scaler.inverse_transform(y_test_for_inverse)\n",
    "\n",
    "    # Cálculo de métricas múltiples\n",
    "    rmse = np.sqrt(np.mean((mean_preds_real - y_test_real) ** 2))\n",
    "    mae = np.mean(np.abs(mean_preds_real - y_test_real))\n",
    "    mape = np.mean(np.abs((y_test_real - mean_preds_real) / y_test_real)) * 100\n",
    "    \n",
    "    metrics = {\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'mape': mape\n",
    "    }\n",
    "    \n",
    "    return metrics, mean_preds_real, std_preds_real, y_test_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01559dc-89ec-4ac4-9a1f-d23366ca7911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8️⃣ Visualizar resultados con gráficos mejorados\n",
    "def plot_predictions(train_data, test_data, predictions, std_dev, title=\"Predicciones del modelo\"):\n",
    "    \"\"\"\n",
    "    Visualiza las predicciones del modelo junto con intervalos de confianza.\n",
    "    Versión mejorada con estilos y anotaciones.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    \n",
    "    # Configurar el eje X\n",
    "    x_train = np.arange(len(train_data))\n",
    "    x_test = np.arange(len(train_data), len(train_data) + len(test_data))\n",
    "    \n",
    "    # Datos de entrenamiento\n",
    "    plt.plot(x_train, train_data, label='Datos históricos', color='#4C72B0', linewidth=1.5, alpha=0.8)\n",
    "    \n",
    "    # Datos de prueba\n",
    "    plt.plot(x_test, test_data, label='Datos reales', color='#55A868', linewidth=2)\n",
    "    \n",
    "    # Predicciones\n",
    "    plt.plot(x_test, predictions, label='Predicciones', color='#C44E52', linewidth=2, linestyle='-')\n",
    "    \n",
    "    # Intervalo de confianza (95%)\n",
    "    plt.fill_between(\n",
    "        x_test,\n",
    "        predictions.flatten() - 1.96 * std_dev.flatten(),\n",
    "        predictions.flatten() + 1.96 * std_dev.flatten(),\n",
    "        alpha=0.2,\n",
    "        color='#C44E52',\n",
    "        label='Intervalo de confianza 95%'\n",
    "    )\n",
    "    \n",
    "    # Marcar división entre entrenamiento y prueba\n",
    "    plt.axvline(x=len(train_data), color='#8172B3', linestyle='--', alpha=0.7, \n",
    "                label='División entrenamiento/prueba')\n",
    "    \n",
    "    # Añadir título y etiquetas\n",
    "    plt.title(title, fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.xlabel('Tiempo', fontsize=12, labelpad=10)\n",
    "    plt.ylabel('Precio', fontsize=12, labelpad=10)\n",
    "    \n",
    "    # Mejorar leyenda\n",
    "    plt.legend(loc='best', frameon=True, fontsize=10)\n",
    "    \n",
    "    # Mejorar diseño\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Añadir anotaciones de métricas si están disponibles\n",
    "    try:\n",
    "        rmse = np.sqrt(np.mean((predictions.flatten() - test_data.flatten()) ** 2))\n",
    "        plt.annotate(f'RMSE: {rmse:.2f}', \n",
    "                     xy=(0.02, 0.05), \n",
    "                     xycoords='axes fraction',\n",
    "                     bbox=dict(boxstyle=\"round,pad=0.3\", fc='white', alpha=0.8))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b54dff7-444d-420e-a1d9-9dcb21ad631d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9️⃣ Ejecución principal - adaptada para un notebook de Jupyter\n",
    "# Configurar dispositivo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Utilizando dispositivo: {device}\")\n",
    "\n",
    "# Configuración\n",
    "ticker = \"AAPL\"\n",
    "start_date = \"2020-01-01\"\n",
    "end_date = \"2024-01-01\"\n",
    "seq_length = 30\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "batch_size = 128\n",
    "num_epochs = 300  # Reducido para ejemplo de notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1c24da-d20c-4226-aa2a-6cc757d41977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener datos\n",
    "print(f\"Descargando datos para {ticker}...\")\n",
    "data = get_stock_data(ticker, start_date, end_date)\n",
    "\n",
    "# Ver primeras filas de los datos\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80886ca-c132-4319-a778-d3d691397a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar datos\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(data.index, data['Close'], label='Precio de Cierre')\n",
    "plt.title(f'Precio de Cierre de {ticker} ({start_date} a {end_date})', fontsize=16)\n",
    "plt.xlabel('Fecha', fontsize=12)\n",
    "plt.ylabel('Precio (USD)', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8843f927-2082-4c4d-9eb5-7034c5016d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar datos\n",
    "print(\"Preparando datos...\")\n",
    "(x_train, y_train), (x_val, y_val), (x_test, y_test), scaler, feature_columns = prepare_data(\n",
    "    data, \n",
    "    seq_length=seq_length, \n",
    "    feature_engineering=True\n",
    ")\n",
    "\n",
    "# Mostrar información sobre los datos\n",
    "print(f\"Forma de x_train: {x_train.shape}\")\n",
    "print(f\"Forma de y_train: {y_train.shape}\")\n",
    "print(f\"Forma de x_val: {x_val.shape}\")\n",
    "print(f\"Forma de y_val: {y_val.shape}\")\n",
    "print(f\"Forma de x_test: {x_test.shape}\")\n",
    "print(f\"Forma de y_test: {y_test.shape}\")\n",
    "print(f\"Características utilizadas: {feature_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b413f869-3946-4fe5-989f-ef1e544e9df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar modelo\n",
    "input_size = x_train.shape[2]  # Número de características\n",
    "\n",
    "model = BayesianLSTM(\n",
    "    input_size=input_size, \n",
    "    hidden_size=hidden_size, \n",
    "    output_size=1,\n",
    "    num_layers=num_layers,\n",
    "    dropout=0.2\n",
    ").to(device)\n",
    "\n",
    "print(f\"Arquitectura del modelo:\")\n",
    "print(model)\n",
    "\n",
    "# Mover datos a dispositivo\n",
    "x_train = x_train.to(device)\n",
    "y_train = y_train.to(device)\n",
    "x_val = x_val.to(device)\n",
    "y_val = y_val.to(device)\n",
    "x_test = x_test.to(device)\n",
    "y_test = y_test.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce31a1cf-c7d8-4fbc-920e-c62e01029f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar modelo (con opción de cargar modelo guardado)\n",
    "model_path = f\"bayesian_lstm_{ticker}.pt\"\n",
    "guide_path = f\"bayesian_guide_{ticker}.pt\"\n",
    "\n",
    "if os.path.exists(model_path) and os.path.exists(guide_path):\n",
    "    print(\"Cargando modelo guardado...\")\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    guide = torch.load(guide_path)\n",
    "else:\n",
    "    print(f\"Entrenando modelo en {device}...\")\n",
    "    losses, guide = train(\n",
    "        model=model,\n",
    "        x_train=x_train,\n",
    "        y_train=y_train,\n",
    "        x_val=x_val,\n",
    "        y_val=y_val,\n",
    "        num_epochs=num_epochs,\n",
    "        batch_size=batch_size,\n",
    "        patience=20,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    \n",
    "    # Guardar modelo\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    torch.save(guide, guide_path)\n",
    "    \n",
    "    # Visualizar curva de aprendizaje\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(losses, label='Pérdida de entrenamiento')\n",
    "    plt.title('Curva de aprendizaje')\n",
    "    plt.xlabel('Época')\n",
    "    plt.ylabel('Pérdida')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f159f4-b3c6-4621-982b-7c3692705d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar modelo\n",
    "print(\"Evaluando modelo...\")\n",
    "metrics, mean_preds, std_preds, y_test_real = evaluate_model(\n",
    "    model=model,\n",
    "    guide=guide,\n",
    "    x_test=x_test,\n",
    "    y_test=y_test,\n",
    "    scaler=scaler,\n",
    "    feature_columns=feature_columns\n",
    ")\n",
    "\n",
    "print(f\"Métricas en conjunto de prueba:\")\n",
    "for name, value in metrics.items():\n",
    "    print(f\"- {name.upper()}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0032b593-64d7-45e6-9bcd-4bb9ab036d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar resultados\n",
    "train_data_real = scaler.inverse_transform(y_train.cpu().numpy().reshape(-1, 1))\n",
    "if len(feature_columns) > 1:\n",
    "    close_idx = feature_columns.index('Close')\n",
    "    train_data_real = train_data_real[:, close_idx].reshape(-1, 1)\n",
    "\n",
    "plt = plot_predictions(\n",
    "    train_data=train_data_real.flatten(),\n",
    "    test_data=y_test_real.flatten(),\n",
    "    predictions=mean_preds.flatten(),\n",
    "    std_dev=std_preds.flatten(),\n",
    "    title=f\"Predicción de precios para {ticker}\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c993f3-68c7-4c67-9271-0ec14a09700d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacer predicción futura\n",
    "print(\"Realizando predicción futura...\")\n",
    "last_sequence = x_test[-1].unsqueeze(0)\n",
    "future_mean, future_std = predict(model, last_sequence, guide, use_parallel=(device.type == 'cpu'))\n",
    "\n",
    "# Preparar para desnormalización\n",
    "future_mean_reshaped = future_mean.reshape(1, -1)\n",
    "future_std_reshaped = future_std.reshape(1, -1)\n",
    "\n",
    "# Procesar para desnormalización\n",
    "if len(feature_columns) > 1:\n",
    "    # Crear arrays dummy completos\n",
    "    dummy_mean = np.zeros((1, len(feature_columns)))\n",
    "    dummy_std = np.zeros((1, len(feature_columns)))\n",
    "    \n",
    "    # Colocar valores solo en la columna de Close\n",
    "    close_idx = feature_columns.index('Close')\n",
    "    dummy_mean[:, close_idx] = future_mean_reshaped\n",
    "    dummy_std[:, close_idx] = future_std_reshaped\n",
    "    \n",
    "    # Desnormalizar\n",
    "    future_mean_price_full = scaler.inverse_transform(dummy_mean)\n",
    "    future_mean_price = future_mean_price_full[:, close_idx].reshape(-1, 1)\n",
    "    \n",
    "    # Para la desviación estándar\n",
    "    dummy_zeros = np.zeros_like(dummy_std)\n",
    "    base = scaler.inverse_transform(dummy_zeros)\n",
    "    with_std = scaler.inverse_transform(dummy_std)\n",
    "    future_std_price = (with_std - base)[:, close_idx].reshape(-1, 1)\n",
    "else:\n",
    "    future_mean_price = scaler.inverse_transform(future_mean_reshaped.reshape(-1, 1))\n",
    "    \n",
    "    # Aproximar el factor de escala para std\n",
    "    dummy_zeros = np.zeros_like(future_std_reshaped.reshape(-1, 1))\n",
    "    base = scaler.inverse_transform(dummy_zeros)\n",
    "    with_std = scaler.inverse_transform(future_std_reshaped.reshape(-1, 1))\n",
    "    future_std_price = with_std - base\n",
    "\n",
    "print(f\"Predicción para el siguiente día: {future_mean_price[0][0]:.2f} ± {future_std_price[0][0]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44cab39-1408-4cde-82fc-615f2c8f617b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analizar incertidumbre de predicciones\n",
    "def generate_prediction_samples(model, x_input, guide, num_samples=1000):\n",
    "    \"\"\"Genera muestras de predicciones para visualizar la distribución\"\"\"\n",
    "    device = x_input.device\n",
    "    predictions = []\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        torch.manual_seed(i)\n",
    "        sampled_model = guide()\n",
    "        sampled_model.to(device)\n",
    "        with torch.no_grad():\n",
    "            pred = sampled_model(x_input).cpu().numpy()\n",
    "            predictions.append(pred)\n",
    "    \n",
    "    predictions = np.array(predictions).flatten()\n",
    "    \n",
    "    # Desnormalizar predicciones\n",
    "    if len(feature_columns) > 1:\n",
    "        # Crear arrays dummy para cada predicción\n",
    "        dummy_preds = np.zeros((len(predictions), len(feature_columns)))\n",
    "        close_idx = feature_columns.index('Close')\n",
    "        dummy_preds[:, close_idx] = predictions\n",
    "        preds_real = scaler.inverse_transform(dummy_preds)[:, close_idx]\n",
    "    else:\n",
    "        preds_real = scaler.inverse_transform(predictions.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    return preds_real\n",
    "\n",
    "# Generar y visualizar distribución de predicciones\n",
    "future_samples = generate_prediction_samples(model, last_sequence, guide)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(future_samples, bins=30, alpha=0.7, color='#4C72B0')\n",
    "plt.axvline(x=future_mean_price[0][0], color='#C44E52', linestyle='--', \n",
    "            label=f'Predicción media: {future_mean_price[0][0]:.2f}')\n",
    "plt.axvline(x=future_mean_price[0][0] - 1.96 * future_std_price[0][0], color='#55A868', linestyle=':', \n",
    "            label=f'Intervalo de confianza 95%')\n",
    "plt.axvline(x=future_mean_price[0][0] + 1.96 * future_std_price[0][0], color='#55A868', linestyle=':')\n",
    "\n",
    "plt.title(f'Distribución de predicciones para {ticker}', fontsize=16)\n",
    "plt.xlabel('Precio predecido', fontsize=12)\n",
    "plt.ylabel('Frecuencia', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Mostrar estadísticas de la distribución\n",
    "percentile_5 = np.percentile(future_samples, 5)\n",
    "percentile_95 = np.percentile(future_samples, 95)\n",
    "print(f\"Intervalo de confianza 90%: [{percentile_5:.2f}, {percentile_95:.2f}]\")\n",
    "print(f\"Probabilidad de que el precio suba: {np.mean(future_samples > y_test_real[-1][0]) * 100:.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
